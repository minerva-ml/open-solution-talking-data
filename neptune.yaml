project: TDAT

name: talking data
tags: [baseline]

metric:
  channel: 'ROC_AUC'
  goal: maximize

# Comment out if not in Cloud Environment
pip-requirements-file: requirements.txt

exclude:
  - output
  - imgs
  - neptune.log
  - offline_job.log
  - .git
  - .idea
  - .ipynb_checkpoints
  - notebooks

parameters:
# Data
  raw_train_filepath: /public/talking_data/data/train.csv
  train_chunks_dir: /public/talking_data/files
  test_filepath:  /public/talking_data/data/test.csv
  experiment_dir: /output/baseline
  train_days: '[7, 8]'
  train_hours: '[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]'
  valid_days: '[9]'
  valid_hours: '[4]'

# Execution
  overwrite: 1
  num_workers: 8
  verbose: 1

# Preprocessing
  target_encoder__min_samples_leaf: 5
  target_encoder__smoothing: 1

# Light GBM
  lgbm_random_search_runs: 0
  lgbm__boosting_type: gbdt
  lgbm__objective: binary
  lgbm__metric: auc
  lgbm__number_boosting_rounds: 100
  lgbm__early_stopping_rounds: 10
  lgbm__learning_rate: 0.01
  lgbm__num_leaves: 10
  lgbm__max_depth: 10
  lgbm__min_child_samples: 10
  lgbm__max_bin: 200
  lgbm__subsample: 0.8
  lgbm__subsample_freq: 6
  lgbm__colsample_bytree: 0.4
  lgbm__min_child_weight: 6
  lgbm__reg_lambda: 0.0
  lgbm__reg_alpha: 0.0
  lgbm__scale_pos_weight: 10
